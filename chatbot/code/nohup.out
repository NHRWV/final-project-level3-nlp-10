epoch no.0  train (0/35796)  loss = 1.31394  avg_loss = 1.31394
epoch no.0  train (200/35796)  loss = 0.66905  avg_loss = 0.86302
epoch no.0  train (400/35796)  loss = 0.58590  avg_loss = 0.71580
epoch no.0  train (600/35796)  loss = 0.79566  avg_loss = 0.68749
epoch no.0  train (800/35796)  loss = 0.80739  avg_loss = 0.68518
epoch no.0  train (1000/35796)  loss = 0.79285  avg_loss = 0.69028
epoch no.0  train (1200/35796)  loss = 0.62170  avg_loss = 0.66400
epoch no.0  train (1400/35796)  loss = 0.73352  avg_loss = 0.68339
epoch no.0  train (1600/35796)  loss = 0.77889  avg_loss = 0.66883
epoch no.0  train (1800/35796)  loss = 0.56398  avg_loss = 0.66122
epoch no.0  train (2000/35796)  loss = 0.63356  avg_loss = 0.65239
epoch no.0  train (2200/35796)  loss = 0.59022  avg_loss = 0.67867
epoch no.0  train (2400/35796)  loss = 0.63696  avg_loss = 0.65121
epoch no.0  train (2600/35796)  loss = 0.81190  avg_loss = 0.64293
epoch no.0  train (2800/35796)  loss = 0.69060  avg_loss = 0.65107
epoch no.0  train (3000/35796)  loss = 0.73479  avg_loss = 0.65668
epoch no.0  train (3200/35796)  loss = 0.68800  avg_loss = 0.64559
epoch no.0  train (3400/35796)  loss = 0.60719  avg_loss = 0.64184
epoch no.0  train (3600/35796)  loss = 0.68893  avg_loss = 0.65001
epoch no.0  train (3800/35796)  loss = 0.66629  avg_loss = 0.66222
epoch no.0  train (4000/35796)  loss = 0.54022  avg_loss = 0.64049
epoch no.0  train (4200/35796)  loss = 0.63387  avg_loss = 0.64967
epoch no.0  train (4400/35796)  loss = 0.59455  avg_loss = 0.64294
epoch no.0  train (4600/35796)  loss = 0.60750  avg_loss = 0.63412
epoch no.0  train (4800/35796)  loss = 0.68682  avg_loss = 0.64259
epoch no.0  train (5000/35796)  loss = 0.68310  avg_loss = 0.63645
epoch no.0  train (5200/35796)  loss = 0.71175  avg_loss = 0.64318
epoch no.0  train (5400/35796)  loss = 0.56951  avg_loss = 0.64445
epoch no.0  train (5600/35796)  loss = 0.56852  avg_loss = 0.62999
epoch no.0  train (5800/35796)  loss = 0.65503  avg_loss = 0.62144
epoch no.0  train (6000/35796)  loss = 0.93530  avg_loss = 0.62427
epoch no.0  train (6200/35796)  loss = 0.63292  avg_loss = 0.62279
epoch no.0  train (6400/35796)  loss = 0.88955  avg_loss = 0.62494
epoch no.0  train (6600/35796)  loss = 0.72757  avg_loss = 0.60719
epoch no.0  train (6800/35796)  loss = 0.56212  avg_loss = 0.61338
epoch no.0  train (7000/35796)  loss = 0.57004  avg_loss = 0.63028
epoch no.0  train (7200/35796)  loss = 0.61601  avg_loss = 0.62053
epoch no.0  train (7400/35796)  loss = 0.46352  avg_loss = 0.62446
epoch no.0  train (7600/35796)  loss = 0.61147  avg_loss = 0.62695
epoch no.0  train (7800/35796)  loss = 0.51326  avg_loss = 0.62402
epoch no.0  train (8000/35796)  loss = 0.68615  avg_loss = 0.62010
epoch no.0  train (8200/35796)  loss = 0.42569  avg_loss = 0.63033
epoch no.0  train (8400/35796)  loss = 0.79494  avg_loss = 0.62147
epoch no.0  train (8600/35796)  loss = 0.68438  avg_loss = 0.63120
epoch no.0  train (8800/35796)  loss = 0.49741  avg_loss = 0.62838
epoch no.0  train (9000/35796)  loss = 0.66190  avg_loss = 0.62932
epoch no.0  train (9200/35796)  loss = 0.53236  avg_loss = 0.62626
epoch no.0  train (9400/35796)  loss = 0.70335  avg_loss = 0.60785
epoch no.0  train (9600/35796)  loss = 0.72827  avg_loss = 0.61466
epoch no.0  train (9800/35796)  loss = 0.60975  avg_loss = 0.61366
epoch no.0  train (10000/35796)  loss = 0.62201  avg_loss = 0.61044
epoch no.0  train (10200/35796)  loss = 0.50836  avg_loss = 0.60710
epoch no.0  train (10400/35796)  loss = 0.44017  avg_loss = 0.61451
epoch no.0  train (10600/35796)  loss = 0.56728  avg_loss = 0.59798
epoch no.0  train (10800/35796)  loss = 0.70546  avg_loss = 0.59330
epoch no.0  train (11000/35796)  loss = 0.56863  avg_loss = 0.62034
epoch no.0  train (11200/35796)  loss = 0.53368  avg_loss = 0.60245
epoch no.0  train (11400/35796)  loss = 0.48536  avg_loss = 0.61727
epoch no.0  train (11600/35796)  loss = 0.55224  avg_loss = 0.60404
epoch no.0  train (11800/35796)  loss = 0.63797  avg_loss = 0.59625
epoch no.0  train (12000/35796)  loss = 0.53687  avg_loss = 0.61235
epoch no.0  train (12200/35796)  loss = 0.65610  avg_loss = 0.63068
epoch no.0  train (12400/35796)  loss = 0.56646  avg_loss = 0.62769
epoch no.0  train (12600/35796)  loss = 0.68942  avg_loss = 0.62198
epoch no.0  train (12800/35796)  loss = 0.68224  avg_loss = 0.61853
epoch no.0  train (13000/35796)  loss = 0.71645  avg_loss = 0.60902
epoch no.0  train (13200/35796)  loss = 0.79153  avg_loss = 0.61010
epoch no.0  train (13400/35796)  loss = 0.52689  avg_loss = 0.59437
epoch no.0  train (13600/35796)  loss = 0.62916  avg_loss = 0.59409
epoch no.0  train (13800/35796)  loss = 0.79943  avg_loss = 0.60220
epoch no.0  train (14000/35796)  loss = 0.71158  avg_loss = 0.58831
epoch no.0  train (14200/35796)  loss = 0.71366  avg_loss = 0.62680
epoch no.0  train (14400/35796)  loss = 0.52543  avg_loss = 0.60773
epoch no.0  train (14600/35796)  loss = 0.52140  avg_loss = 0.60433
epoch no.0  train (14800/35796)  loss = 0.55924  avg_loss = 0.61572
epoch no.0  train (15000/35796)  loss = 0.53780  avg_loss = 0.60133
epoch no.0  train (15200/35796)  loss = 0.54549  avg_loss = 0.60239
epoch no.0  train (15400/35796)  loss = 0.84528  avg_loss = 0.61052
epoch no.0  train (15600/35796)  loss = 0.65182  avg_loss = 0.59050
epoch no.0  train (15800/35796)  loss = 0.43229  avg_loss = 0.58186
epoch no.0  train (16000/35796)  loss = 0.63901  avg_loss = 0.60472
epoch no.0  train (16200/35796)  loss = 0.75351  avg_loss = 0.59749
epoch no.0  train (16400/35796)  loss = 0.54123  avg_loss = 0.60600
epoch no.0  train (16600/35796)  loss = 0.52538  avg_loss = 0.58734
epoch no.0  train (16800/35796)  loss = 0.70004  avg_loss = 0.59199
epoch no.0  train (17000/35796)  loss = 0.55886  avg_loss = 0.60838
epoch no.0  train (17200/35796)  loss = 0.84605  avg_loss = 0.59574
epoch no.0  train (17400/35796)  loss = 0.59391  avg_loss = 0.59634
epoch no.0  train (17600/35796)  loss = 0.60960  avg_loss = 0.59938
epoch no.0  train (17800/35796)  loss = 0.55201  avg_loss = 0.58783
epoch no.0  train (18000/35796)  loss = 0.63135  avg_loss = 0.58565
epoch no.0  train (18200/35796)  loss = 0.90376  avg_loss = 0.59205
epoch no.0  train (18400/35796)  loss = 0.66290  avg_loss = 0.59498
epoch no.0  train (18600/35796)  loss = 0.48693  avg_loss = 0.59917
epoch no.0  train (18800/35796)  loss = 0.44454  avg_loss = 0.58747
epoch no.0  train (19000/35796)  loss = 0.70067  avg_loss = 0.60336
epoch no.0  train (19200/35796)  loss = 0.39438  avg_loss = 0.59589
epoch no.0  train (19400/35796)  loss = 0.86916  avg_loss = 0.59552
epoch no.0  train (19600/35796)  loss = 0.63476  avg_loss = 0.57798
epoch no.0  train (19800/35796)  loss = 0.42772  avg_loss = 0.59511
epoch no.0  train (20000/35796)  loss = 0.75139  avg_loss = 0.58596
epoch no.0  train (20200/35796)  loss = 0.53440  avg_loss = 0.59983
epoch no.0  train (20400/35796)  loss = 0.50271  avg_loss = 0.60526
epoch no.0  train (20600/35796)  loss = 0.80839  avg_loss = 0.60568
epoch no.0  train (20800/35796)  loss = 0.72405  avg_loss = 0.58189
epoch no.0  train (21000/35796)  loss = 0.40861  avg_loss = 0.58552
epoch no.0  train (21200/35796)  loss = 0.83929  avg_loss = 0.59496
epoch no.0  train (21400/35796)  loss = 0.63754  avg_loss = 0.58151
epoch no.0  train (21600/35796)  loss = 0.42735  avg_loss = 0.59611
epoch no.0  train (21800/35796)  loss = 0.58633  avg_loss = 0.58944
epoch no.0  train (22000/35796)  loss = 0.55794  avg_loss = 0.57799
epoch no.0  train (22200/35796)  loss = 0.65470  avg_loss = 0.58877
epoch no.0  train (22400/35796)  loss = 0.65307  avg_loss = 0.59079
epoch no.0  train (22600/35796)  loss = 0.59860  avg_loss = 0.59024
epoch no.0  train (22800/35796)  loss = 0.65174  avg_loss = 0.58371
epoch no.0  train (23000/35796)  loss = 0.37684  avg_loss = 0.59697
epoch no.0  train (23200/35796)  loss = 0.63548  avg_loss = 0.58544
epoch no.0  train (23400/35796)  loss = 0.43952  avg_loss = 0.57944
epoch no.0  train (23600/35796)  loss = 0.93054  avg_loss = 0.59465
epoch no.0  train (23800/35796)  loss = 0.37128  avg_loss = 0.58676
epoch no.0  train (24000/35796)  loss = 0.80278  avg_loss = 0.58712
epoch no.0  train (24200/35796)  loss = 0.68274  avg_loss = 0.58520
epoch no.0  train (24400/35796)  loss = 0.54732  avg_loss = 0.60999
epoch no.0  train (24600/35796)  loss = 0.75553  avg_loss = 0.59742
epoch no.0  train (24800/35796)  loss = 0.38981  avg_loss = 0.59303
epoch no.0  train (25000/35796)  loss = 0.67390  avg_loss = 0.57036
epoch no.0  train (25200/35796)  loss = 0.51057  avg_loss = 0.57807
epoch no.0  train (25400/35796)  loss = 0.42478  avg_loss = 0.58237
epoch no.0  train (25600/35796)  loss = 0.63951  avg_loss = 0.59830
epoch no.0  train (25800/35796)  loss = 0.66265  avg_loss = 0.59077
epoch no.0  train (26000/35796)  loss = 0.59255  avg_loss = 0.58576
epoch no.0  train (26200/35796)  loss = 0.57895  avg_loss = 0.58312
epoch no.0  train (26400/35796)  loss = 0.59492  avg_loss = 0.58186
epoch no.0  train (26600/35796)  loss = 0.57965  avg_loss = 0.59550
epoch no.0  train (26800/35796)  loss = 0.63410  avg_loss = 0.58356
epoch no.0  train (27000/35796)  loss = 0.65999  avg_loss = 0.58756
epoch no.0  train (27200/35796)  loss = 0.71779  avg_loss = 0.58826
epoch no.0  train (27400/35796)  loss = 0.98042  avg_loss = 0.57761
epoch no.0  train (27600/35796)  loss = 0.51581  avg_loss = 0.57110
epoch no.0  train (27800/35796)  loss = 0.46189  avg_loss = 0.58011
epoch no.0  train (28000/35796)  loss = 0.70724  avg_loss = 0.57865
epoch no.0  train (28200/35796)  loss = 0.50129  avg_loss = 0.57882
epoch no.0  train (28400/35796)  loss = 0.53967  avg_loss = 0.58137
epoch no.0  train (28600/35796)  loss = 0.59071  avg_loss = 0.57919
epoch no.0  train (28800/35796)  loss = 0.68757  avg_loss = 0.59504
epoch no.0  train (29000/35796)  loss = 0.44118  avg_loss = 0.58654
epoch no.0  train (29200/35796)  loss = 0.48460  avg_loss = 0.58564
epoch no.0  train (29400/35796)  loss = 0.65504  avg_loss = 0.57545
epoch no.0  train (29600/35796)  loss = 0.64228  avg_loss = 0.58364
epoch no.0  train (29800/35796)  loss = 0.57382  avg_loss = 0.57696
epoch no.0  train (30000/35796)  loss = 0.44308  avg_loss = 0.58385
epoch no.0  train (30200/35796)  loss = 0.46120  avg_loss = 0.58162
epoch no.0  train (30400/35796)  loss = 0.67502  avg_loss = 0.59567
epoch no.0  train (30600/35796)  loss = 0.49387  avg_loss = 0.57020
epoch no.0  train (30800/35796)  loss = 0.70197  avg_loss = 0.58705
epoch no.0  train (31000/35796)  loss = 0.43030  avg_loss = 0.58829
epoch no.0  train (31200/35796)  loss = 0.52692  avg_loss = 0.58547
epoch no.0  train (31400/35796)  loss = 0.38732  avg_loss = 0.57201
epoch no.0  train (31600/35796)  loss = 0.48787  avg_loss = 0.56410
epoch no.0  train (31800/35796)  loss = 0.69027  avg_loss = 0.56004
epoch no.0  train (32000/35796)  loss = 0.47182  avg_loss = 0.57258
epoch no.0  train (32200/35796)  loss = 0.47177  avg_loss = 0.58467
epoch no.0  train (32400/35796)  loss = 0.56019  avg_loss = 0.57062
epoch no.0  train (32600/35796)  loss = 0.54280  avg_loss = 0.57763
epoch no.0  train (32800/35796)  loss = 0.55019  avg_loss = 0.58792
epoch no.0  train (33000/35796)  loss = 0.76583  avg_loss = 0.57766
epoch no.0  train (33200/35796)  loss = 0.52409  avg_loss = 0.56557
epoch no.0  train (33400/35796)  loss = 0.49401  avg_loss = 0.58035
epoch no.0  train (33600/35796)  loss = 0.56592  avg_loss = 0.57080
epoch no.0  train (33800/35796)  loss = 0.42950  avg_loss = 0.57386
epoch no.0  train (34000/35796)  loss = 0.62763  avg_loss = 0.56349
epoch no.0  train (34200/35796)  loss = 0.37162  avg_loss = 0.58514
epoch no.0  train (34400/35796)  loss = 0.35963  avg_loss = 0.57678
epoch no.0  train (34600/35796)  loss = 0.72903  avg_loss = 0.59870
epoch no.0  train (34800/35796)  loss = 0.63197  avg_loss = 0.57119
epoch no.0  train (35000/35796)  loss = 0.58415  avg_loss = 0.56588
epoch no.0  train (35200/35796)  loss = 0.63715  avg_loss = 0.57042
epoch no.0  train (35400/35796)  loss = 0.60020  avg_loss = 0.57715
epoch no.0  train (35600/35796)  loss = 0.63032  avg_loss = 0.59205
epoch no.1  train (0/35796)  loss = 0.45577  avg_loss = 0.56952
epoch no.1  train (200/35796)  loss = 0.65655  avg_loss = 0.52728
epoch no.1  train (400/35796)  loss = 0.49997  avg_loss = 0.54119
epoch no.1  train (600/35796)  loss = 0.55356  avg_loss = 0.54041
epoch no.1  train (800/35796)  loss = 0.52813  avg_loss = 0.53359
epoch no.1  train (1000/35796)  loss = 0.51346  avg_loss = 0.53025
epoch no.1  train (1200/35796)  loss = 0.41961  avg_loss = 0.53154
epoch no.1  train (1400/35796)  loss = 0.69720  avg_loss = 0.53632
epoch no.1  train (1600/35796)  loss = 0.45778  avg_loss = 0.55330
epoch no.1  train (1800/35796)  loss = 0.57259  avg_loss = 0.53039
epoch no.1  train (2000/35796)  loss = 0.54142  avg_loss = 0.53392
epoch no.1  train (2200/35796)  loss = 0.78575  avg_loss = 0.54417
epoch no.1  train (2400/35796)  loss = 0.54427  avg_loss = 0.54182
epoch no.1  train (2600/35796)  loss = 0.55047  avg_loss = 0.54959
epoch no.1  train (2800/35796)  loss = 0.55992  avg_loss = 0.53249
epoch no.1  train (3000/35796)  loss = 0.51743  avg_loss = 0.53265
epoch no.1  train (3200/35796)  loss = 0.59169  avg_loss = 0.52665
epoch no.1  train (3400/35796)  loss = 0.48099  avg_loss = 0.52822
epoch no.1  train (3600/35796)  loss = 0.61680  avg_loss = 0.53491
epoch no.1  train (3800/35796)  loss = 0.36680  avg_loss = 0.53372
epoch no.1  train (4000/35796)  loss = 0.32301  avg_loss = 0.53902
epoch no.1  train (4200/35796)  loss = 0.56381  avg_loss = 0.53476
epoch no.1  train (4400/35796)  loss = 0.48378  avg_loss = 0.51982
epoch no.1  train (4600/35796)  loss = 0.55106  avg_loss = 0.53286
epoch no.1  train (4800/35796)  loss = 0.50524  avg_loss = 0.53832
epoch no.1  train (5000/35796)  loss = 0.41041  avg_loss = 0.53231
epoch no.1  train (5200/35796)  loss = 0.54574  avg_loss = 0.53016
epoch no.1  train (5400/35796)  loss = 0.59222  avg_loss = 0.53481
epoch no.1  train (5600/35796)  loss = 0.37755  avg_loss = 0.54268
epoch no.1  train (5800/35796)  loss = 0.42680  avg_loss = 0.54887
epoch no.1  train (6000/35796)  loss = 0.57141  avg_loss = 0.55066
epoch no.1  train (6200/35796)  loss = 0.59183  avg_loss = 0.53108
epoch no.1  train (6400/35796)  loss = 0.47494  avg_loss = 0.54274
epoch no.1  train (6600/35796)  loss = 0.55780  avg_loss = 0.53817
epoch no.1  train (6800/35796)  loss = 0.59346  avg_loss = 0.52309
epoch no.1  train (7000/35796)  loss = 0.44404  avg_loss = 0.53123
epoch no.1  train (7200/35796)  loss = 0.61170  avg_loss = 0.53745
epoch no.1  train (7400/35796)  loss = 0.38442  avg_loss = 0.53572
epoch no.1  train (7600/35796)  loss = 0.37380  avg_loss = 0.52536
epoch no.1  train (7800/35796)  loss = 0.55646  avg_loss = 0.53487
epoch no.1  train (8000/35796)  loss = 0.62257  avg_loss = 0.53175
epoch no.1  train (8200/35796)  loss = 0.41507  avg_loss = 0.53561
epoch no.1  train (8400/35796)  loss = 0.56060  avg_loss = 0.53595
epoch no.1  train (8600/35796)  loss = 0.37144  avg_loss = 0.54075
epoch no.1  train (8800/35796)  loss = 0.35606  avg_loss = 0.53990
epoch no.1  train (9000/35796)  loss = 0.44247  avg_loss = 0.53605
epoch no.1  train (9200/35796)  loss = 0.70821  avg_loss = 0.53820
epoch no.1  train (9400/35796)  loss = 0.66487  avg_loss = 0.52052
epoch no.1  train (9600/35796)  loss = 0.48769  avg_loss = 0.53541
epoch no.1  train (9800/35796)  loss = 0.33529  avg_loss = 0.51107
epoch no.1  train (10000/35796)  loss = 0.45783  avg_loss = 0.53783
epoch no.1  train (10200/35796)  loss = 0.53245  avg_loss = 0.53996
epoch no.1  train (10400/35796)  loss = 0.56029  avg_loss = 0.53369
epoch no.1  train (10600/35796)  loss = 0.32719  avg_loss = 0.53720
epoch no.1  train (10800/35796)  loss = 0.76899  avg_loss = 0.53365
epoch no.1  train (11000/35796)  loss = 0.58005  avg_loss = 0.52141
epoch no.1  train (11200/35796)  loss = 0.46897  avg_loss = 0.52164
epoch no.1  train (11400/35796)  loss = 0.53580  avg_loss = 0.54183
epoch no.1  train (11600/35796)  loss = 0.56913  avg_loss = 0.54035
epoch no.1  train (11800/35796)  loss = 0.56030  avg_loss = 0.52662
epoch no.1  train (12000/35796)  loss = 0.43583  avg_loss = 0.53930
epoch no.1  train (12200/35796)  loss = 0.50063  avg_loss = 0.54824
epoch no.1  train (12400/35796)  loss = 0.52536  avg_loss = 0.53374
epoch no.1  train (12600/35796)  loss = 0.72919  avg_loss = 0.53983
epoch no.1  train (12800/35796)  loss = 0.43167  avg_loss = 0.53989
epoch no.1  train (13000/35796)  loss = 0.51919  avg_loss = 0.52595
epoch no.1  train (13200/35796)  loss = 0.75002  avg_loss = 0.53159
epoch no.1  train (13400/35796)  loss = 0.66236  avg_loss = 0.54166
epoch no.1  train (13600/35796)  loss = 0.52346  avg_loss = 0.53188
epoch no.1  train (13800/35796)  loss = 0.59115  avg_loss = 0.52687
epoch no.1  train (14000/35796)  loss = 0.48280  avg_loss = 0.53652
epoch no.1  train (14200/35796)  loss = 0.53686  avg_loss = 0.54813
epoch no.1  train (14400/35796)  loss = 0.57343  avg_loss = 0.52714
epoch no.1  train (14600/35796)  loss = 0.65106  avg_loss = 0.53758
epoch no.1  train (14800/35796)  loss = 0.46807  avg_loss = 0.53223
epoch no.1  train (15000/35796)  loss = 0.46455  avg_loss = 0.53416
epoch no.1  train (15200/35796)  loss = 0.61510  avg_loss = 0.53810
epoch no.1  train (15400/35796)  loss = 0.48683  avg_loss = 0.51590
epoch no.1  train (15600/35796)  loss = 0.41760  avg_loss = 0.53766
epoch no.1  train (15800/35796)  loss = 0.46204  avg_loss = 0.53362
epoch no.1  train (16000/35796)  loss = 0.43799  avg_loss = 0.52217
epoch no.1  train (16200/35796)  loss = 0.30437  avg_loss = 0.51791
epoch no.1  train (16400/35796)  loss = 0.61787  avg_loss = 0.52068
epoch no.1  train (16600/35796)  loss = 0.69569  avg_loss = 0.53023
epoch no.1  train (16800/35796)  loss = 0.44383  avg_loss = 0.52127
epoch no.1  train (17000/35796)  loss = 0.51441  avg_loss = 0.53762
epoch no.1  train (17200/35796)  loss = 0.49387  avg_loss = 0.52546
epoch no.1  train (17400/35796)  loss = 0.33672  avg_loss = 0.51808
epoch no.1  train (17600/35796)  loss = 0.60044  avg_loss = 0.52196
epoch no.1  train (17800/35796)  loss = 0.61848  avg_loss = 0.52900
epoch no.1  train (18000/35796)  loss = 0.56224  avg_loss = 0.52422
epoch no.1  train (18200/35796)  loss = 0.55034  avg_loss = 0.52232
epoch no.1  train (18400/35796)  loss = 0.46720  avg_loss = 0.53372
epoch no.1  train (18600/35796)  loss = 0.63110  avg_loss = 0.53509
epoch no.1  train (18800/35796)  loss = 0.49654  avg_loss = 0.51404
epoch no.1  train (19000/35796)  loss = 0.76340  avg_loss = 0.51857
epoch no.1  train (19200/35796)  loss = 0.63204  avg_loss = 0.53625
epoch no.1  train (19400/35796)  loss = 0.48188  avg_loss = 0.54591
epoch no.1  train (19600/35796)  loss = 0.59044  avg_loss = 0.54450
epoch no.1  train (19800/35796)  loss = 0.54322  avg_loss = 0.53875
epoch no.1  train (20000/35796)  loss = 0.42927  avg_loss = 0.54156
epoch no.1  train (20200/35796)  loss = 0.52074  avg_loss = 0.52618
epoch no.1  train (20400/35796)  loss = 0.59331  avg_loss = 0.52952
epoch no.1  train (20600/35796)  loss = 0.53613  avg_loss = 0.52229
epoch no.1  train (20800/35796)  loss = 0.64484  avg_loss = 0.53598
epoch no.1  train (21000/35796)  loss = 0.45384  avg_loss = 0.53359
epoch no.1  train (21200/35796)  loss = 0.76346  avg_loss = 0.52958
epoch no.1  train (21400/35796)  loss = 0.72612  avg_loss = 0.54123
epoch no.1  train (21600/35796)  loss = 0.56321  avg_loss = 0.53774
epoch no.1  train (21800/35796)  loss = 0.45504  avg_loss = 0.53696
epoch no.1  train (22000/35796)  loss = 0.84298  avg_loss = 0.51806
epoch no.1  train (22200/35796)  loss = 0.61492  avg_loss = 0.53426
epoch no.1  train (22400/35796)  loss = 0.50074  avg_loss = 0.51768
epoch no.1  train (22600/35796)  loss = 0.39875  avg_loss = 0.53261
epoch no.1  train (22800/35796)  loss = 0.68043  avg_loss = 0.53687
epoch no.1  train (23000/35796)  loss = 0.50023  avg_loss = 0.54623
epoch no.1  train (23200/35796)  loss = 0.48527  avg_loss = 0.53752
epoch no.1  train (23400/35796)  loss = 0.58521  avg_loss = 0.51918
epoch no.1  train (23600/35796)  loss = 0.52232  avg_loss = 0.54573
epoch no.1  train (23800/35796)  loss = 0.39269  avg_loss = 0.53140
epoch no.1  train (24000/35796)  loss = 0.45634  avg_loss = 0.52409
epoch no.1  train (24200/35796)  loss = 0.68226  avg_loss = 0.52057
epoch no.1  train (24400/35796)  loss = 0.55381  avg_loss = 0.52027
epoch no.1  train (24600/35796)  loss = 0.47673  avg_loss = 0.52012
epoch no.1  train (24800/35796)  loss = 0.60138  avg_loss = 0.53029
epoch no.1  train (25000/35796)  loss = 0.54247  avg_loss = 0.53261
epoch no.1  train (25200/35796)  loss = 0.60512  avg_loss = 0.52569
epoch no.1  train (25400/35796)  loss = 0.63032  avg_loss = 0.51942
epoch no.1  train (25600/35796)  loss = 0.68186  avg_loss = 0.52409
epoch no.1  train (25800/35796)  loss = 0.41523  avg_loss = 0.54268
epoch no.1  train (26000/35796)  loss = 0.48879  avg_loss = 0.53139
epoch no.1  train (26200/35796)  loss = 0.32727  avg_loss = 0.53686
epoch no.1  train (26400/35796)  loss = 0.61066  avg_loss = 0.52984
epoch no.1  train (26600/35796)  loss = 0.55794  avg_loss = 0.54322
epoch no.1  train (26800/35796)  loss = 0.41097  avg_loss = 0.53100
epoch no.1  train (27000/35796)  loss = 0.67929  avg_loss = 0.53629
epoch no.1  train (27200/35796)  loss = 0.43489  avg_loss = 0.52892
epoch no.1  train (27400/35796)  loss = 0.57226  avg_loss = 0.54198
epoch no.1  train (27600/35796)  loss = 0.50298  avg_loss = 0.53711
epoch no.1  train (27800/35796)  loss = 0.47322  avg_loss = 0.52801
epoch no.1  train (28000/35796)  loss = 0.35714  avg_loss = 0.52271
epoch no.1  train (28200/35796)  loss = 0.71154  avg_loss = 0.52332
epoch no.1  train (28400/35796)  loss = 0.53200  avg_loss = 0.51492
epoch no.1  train (28600/35796)  loss = 0.43024  avg_loss = 0.52091
epoch no.1  train (28800/35796)  loss = 0.59309  avg_loss = 0.52899
epoch no.1  train (29000/35796)  loss = 0.69334  avg_loss = 0.52906
epoch no.1  train (29200/35796)  loss = 0.68390  avg_loss = 0.52456
epoch no.1  train (29400/35796)  loss = 0.50560  avg_loss = 0.52570
epoch no.1  train (29600/35796)  loss = 0.46908  avg_loss = 0.52897
epoch no.1  train (29800/35796)  loss = 0.82343  avg_loss = 0.53913
epoch no.1  train (30000/35796)  loss = 0.55296  avg_loss = 0.54485
epoch no.1  train (30200/35796)  loss = 0.47845  avg_loss = 0.53216
epoch no.1  train (30400/35796)  loss = 0.57424  avg_loss = 0.53733
epoch no.1  train (30600/35796)  loss = 0.56910  avg_loss = 0.51963
epoch no.1  train (30800/35796)  loss = 0.65628  avg_loss = 0.52629
epoch no.1  train (31000/35796)  loss = 0.33460  avg_loss = 0.51154
epoch no.1  train (31200/35796)  loss = 0.39392  avg_loss = 0.52485
epoch no.1  train (31400/35796)  loss = 0.64348  avg_loss = 0.53243
epoch no.1  train (31600/35796)  loss = 0.36845  avg_loss = 0.52147
epoch no.1  train (31800/35796)  loss = 0.45598  avg_loss = 0.51086
epoch no.1  train (32000/35796)  loss = 0.58912  avg_loss = 0.51569
epoch no.1  train (32200/35796)  loss = 0.74761  avg_loss = 0.52244
epoch no.1  train (32400/35796)  loss = 0.51867  avg_loss = 0.53295
epoch no.1  train (32600/35796)  loss = 0.47344  avg_loss = 0.52460
epoch no.1  train (32800/35796)  loss = 0.55451  avg_loss = 0.53483
epoch no.1  train (33000/35796)  loss = 0.50112  avg_loss = 0.52373
epoch no.1  train (33200/35796)  loss = 0.45852  avg_loss = 0.52671
epoch no.1  train (33400/35796)  loss = 0.52017  avg_loss = 0.52419
epoch no.1  train (33600/35796)  loss = 0.56875  avg_loss = 0.53231
epoch no.1  train (33800/35796)  loss = 0.64310  avg_loss = 0.52720
epoch no.1  train (34000/35796)  loss = 0.56273  avg_loss = 0.53621
epoch no.1  train (34200/35796)  loss = 0.45292  avg_loss = 0.53397
epoch no.1  train (34400/35796)  loss = 0.52129  avg_loss = 0.53445
epoch no.1  train (34600/35796)  loss = 0.58799  avg_loss = 0.51995
epoch no.1  train (34800/35796)  loss = 0.53024  avg_loss = 0.52926
epoch no.1  train (35000/35796)  loss = 0.43239  avg_loss = 0.52299
epoch no.1  train (35200/35796)  loss = 0.50891  avg_loss = 0.53331
epoch no.1  train (35400/35796)  loss = 0.55925  avg_loss = 0.51721
epoch no.1  train (35600/35796)  loss = 0.80176  avg_loss = 0.53137
epoch no.2  train (0/35796)  loss = 0.52790  avg_loss = 0.52431
epoch no.2  train (200/35796)  loss = 0.52510  avg_loss = 0.47927
epoch no.2  train (400/35796)  loss = 0.39158  avg_loss = 0.48113
epoch no.2  train (600/35796)  loss = 0.50983  avg_loss = 0.47225
epoch no.2  train (800/35796)  loss = 0.45205  avg_loss = 0.46634
epoch no.2  train (1000/35796)  loss = 0.57155  avg_loss = 0.47986
epoch no.2  train (1200/35796)  loss = 0.40996  avg_loss = 0.46584
epoch no.2  train (1400/35796)  loss = 0.33837  avg_loss = 0.46913
epoch no.2  train (1600/35796)  loss = 0.37146  avg_loss = 0.47621
epoch no.2  train (1800/35796)  loss = 0.32104  avg_loss = 0.46815
epoch no.2  train (2000/35796)  loss = 0.51508  avg_loss = 0.46644
epoch no.2  train (2200/35796)  loss = 0.49502  avg_loss = 0.47427
epoch no.2  train (2400/35796)  loss = 0.54575  avg_loss = 0.46318
epoch no.2  train (2600/35796)  loss = 0.42987  avg_loss = 0.46132
epoch no.2  train (2800/35796)  loss = 0.34605  avg_loss = 0.47133
epoch no.2  train (3000/35796)  loss = 0.50617  avg_loss = 0.47445
epoch no.2  train (3200/35796)  loss = 0.57366  avg_loss = 0.47677
epoch no.2  train (3400/35796)  loss = 0.53875  avg_loss = 0.47569
epoch no.2  train (3600/35796)  loss = 0.56947  avg_loss = 0.46672
epoch no.2  train (3800/35796)  loss = 0.41279  avg_loss = 0.46112
epoch no.2  train (4000/35796)  loss = 0.31723  avg_loss = 0.46504
epoch no.2  train (4200/35796)  loss = 0.42022  avg_loss = 0.47787
epoch no.2  train (4400/35796)  loss = 0.37620  avg_loss = 0.47639
epoch no.2  train (4600/35796)  loss = 0.61886  avg_loss = 0.45354
epoch no.2  train (4800/35796)  loss = 0.64431  avg_loss = 0.46637
epoch no.2  train (5000/35796)  loss = 0.42283  avg_loss = 0.45986
epoch no.2  train (5200/35796)  loss = 0.30177  avg_loss = 0.47696
epoch no.2  train (5400/35796)  loss = 0.55842  avg_loss = 0.47322
epoch no.2  train (5600/35796)  loss = 0.44437  avg_loss = 0.46942
epoch no.2  train (5800/35796)  loss = 0.47868  avg_loss = 0.46957
epoch no.2  train (6000/35796)  loss = 0.35562  avg_loss = 0.46641
epoch no.2  train (6200/35796)  loss = 0.33861  avg_loss = 0.46842
epoch no.2  train (6400/35796)  loss = 0.39552  avg_loss = 0.46378
epoch no.2  train (6600/35796)  loss = 0.73209  avg_loss = 0.47715
epoch no.2  train (6800/35796)  loss = 0.46855  avg_loss = 0.47223
epoch no.2  train (7000/35796)  loss = 0.38565  avg_loss = 0.46533
epoch no.2  train (7200/35796)  loss = 0.45258  avg_loss = 0.48479
epoch no.2  train (7400/35796)  loss = 0.53065  avg_loss = 0.47518
epoch no.2  train (7600/35796)  loss = 0.50119  avg_loss = 0.46922
epoch no.2  train (7800/35796)  loss = 0.39169  avg_loss = 0.46129
epoch no.2  train (8000/35796)  loss = 0.35267  avg_loss = 0.46037
epoch no.2  train (8200/35796)  loss = 0.62260  avg_loss = 0.46182
epoch no.2  train (8400/35796)  loss = 0.46986  avg_loss = 0.46478
epoch no.2  train (8600/35796)  loss = 0.45771  avg_loss = 0.46231
epoch no.2  train (8800/35796)  loss = 0.59075  avg_loss = 0.47680
epoch no.2  train (9000/35796)  loss = 0.50243  avg_loss = 0.48544
epoch no.2  train (9200/35796)  loss = 0.38962  avg_loss = 0.48205
epoch no.2  train (9400/35796)  loss = 0.53452  avg_loss = 0.47755
epoch no.2  train (9600/35796)  loss = 0.46716  avg_loss = 0.47061
epoch no.2  train (9800/35796)  loss = 0.33276  avg_loss = 0.46970
epoch no.2  train (10000/35796)  loss = 0.57608  avg_loss = 0.46722
epoch no.2  train (10200/35796)  loss = 0.53469  avg_loss = 0.46342
epoch no.2  train (10400/35796)  loss = 0.43809  avg_loss = 0.48590
epoch no.2  train (10600/35796)  loss = 0.37298  avg_loss = 0.46211
epoch no.2  train (10800/35796)  loss = 0.56593  avg_loss = 0.46993
epoch no.2  train (11000/35796)  loss = 0.47402  avg_loss = 0.47647
epoch no.2  train (11200/35796)  loss = 0.56831  avg_loss = 0.46772
epoch no.2  train (11400/35796)  loss = 0.46741  avg_loss = 0.48055
epoch no.2  train (11600/35796)  loss = 0.56745  avg_loss = 0.47380
epoch no.2  train (11800/35796)  loss = 0.40423  avg_loss = 0.46783
epoch no.2  train (12000/35796)  loss = 0.63302  avg_loss = 0.47534
epoch no.2  train (12200/35796)  loss = 0.46214  avg_loss = 0.46976
epoch no.2  train (12400/35796)  loss = 0.46072  avg_loss = 0.46859
epoch no.2  train (12600/35796)  loss = 0.64753  avg_loss = 0.47318
epoch no.2  train (12800/35796)  loss = 0.58582  avg_loss = 0.47516
epoch no.2  train (13000/35796)  loss = 0.45626  avg_loss = 0.47231
epoch no.2  train (13200/35796)  loss = 0.24478  avg_loss = 0.47226
epoch no.2  train (13400/35796)  loss = 0.40409  avg_loss = 0.46927
epoch no.2  train (13600/35796)  loss = 0.40647  avg_loss = 0.46833
epoch no.2  train (13800/35796)  loss = 0.46675  avg_loss = 0.47820
epoch no.2  train (14000/35796)  loss = 0.43977  avg_loss = 0.48297
epoch no.2  train (14200/35796)  loss = 0.45194  avg_loss = 0.48200
epoch no.2  train (14400/35796)  loss = 0.57943  avg_loss = 0.48511
epoch no.2  train (14600/35796)  loss = 0.35811  avg_loss = 0.46928
epoch no.2  train (14800/35796)  loss = 0.48088  avg_loss = 0.47359
epoch no.2  train (15000/35796)  loss = 0.40093  avg_loss = 0.47656
epoch no.2  train (15200/35796)  loss = 0.55282  avg_loss = 0.46706
epoch no.2  train (15400/35796)  loss = 0.33336  avg_loss = 0.45933
epoch no.2  train (15600/35796)  loss = 0.34865  avg_loss = 0.46905
epoch no.2  train (15800/35796)  loss = 0.47523  avg_loss = 0.47642
epoch no.2  train (16000/35796)  loss = 0.38748  avg_loss = 0.47538
epoch no.2  train (16200/35796)  loss = 0.54934  avg_loss = 0.47122
epoch no.2  train (16400/35796)  loss = 0.49733  avg_loss = 0.48368
epoch no.2  train (16600/35796)  loss = 0.42003  avg_loss = 0.47484
epoch no.2  train (16800/35796)  loss = 0.50566  avg_loss = 0.46211
epoch no.2  train (17000/35796)  loss = 0.56071  avg_loss = 0.47468
epoch no.2  train (17200/35796)  loss = 0.66192  avg_loss = 0.47206
epoch no.2  train (17400/35796)  loss = 0.50624  avg_loss = 0.47548
epoch no.2  train (17600/35796)  loss = 0.38417  avg_loss = 0.46571
epoch no.2  train (17800/35796)  loss = 0.45289  avg_loss = 0.48187
epoch no.2  train (18000/35796)  loss = 0.43140  avg_loss = 0.48353
epoch no.2  train (18200/35796)  loss = 0.42052  avg_loss = 0.47333
epoch no.2  train (18400/35796)  loss = 0.26798  avg_loss = 0.47900
epoch no.2  train (18600/35796)  loss = 0.49000  avg_loss = 0.47441
epoch no.2  train (18800/35796)  loss = 0.61677  avg_loss = 0.48150
epoch no.2  train (19000/35796)  loss = 0.34210  avg_loss = 0.46416
epoch no.2  train (19200/35796)  loss = 0.36124  avg_loss = 0.47057
epoch no.2  train (19400/35796)  loss = 0.47256  avg_loss = 0.47321
epoch no.2  train (19600/35796)  loss = 0.53938  avg_loss = 0.46722
epoch no.2  train (19800/35796)  loss = 0.53798  avg_loss = 0.48483
epoch no.2  train (20000/35796)  loss = 0.46417  avg_loss = 0.48291
epoch no.2  train (20200/35796)  loss = 0.44663  avg_loss = 0.46469
epoch no.2  train (20400/35796)  loss = 0.37315  avg_loss = 0.46278
epoch no.2  train (20600/35796)  loss = 0.50888  avg_loss = 0.47665
epoch no.2  train (20800/35796)  loss = 0.52345  avg_loss = 0.47025
epoch no.2  train (21000/35796)  loss = 0.51030  avg_loss = 0.47167
epoch no.2  train (21200/35796)  loss = 0.51866  avg_loss = 0.46612
epoch no.2  train (21400/35796)  loss = 0.52564  avg_loss = 0.46603
epoch no.2  train (21600/35796)  loss = 0.44873  avg_loss = 0.47284
epoch no.2  train (21800/35796)  loss = 0.34599  avg_loss = 0.48237
epoch no.2  train (22000/35796)  loss = 0.44550  avg_loss = 0.48854
epoch no.2  train (22200/35796)  loss = 0.34931  avg_loss = 0.47474
epoch no.2  train (22400/35796)  loss = 0.35906  avg_loss = 0.47618
epoch no.2  train (22600/35796)  loss = 0.44702  avg_loss = 0.47421
epoch no.2  train (22800/35796)  loss = 0.36799  avg_loss = 0.46504
epoch no.2  train (23000/35796)  loss = 0.27615  avg_loss = 0.48747
epoch no.2  train (23200/35796)  loss = 0.59829  avg_loss = 0.47344
epoch no.2  train (23400/35796)  loss = 0.48311  avg_loss = 0.46565
epoch no.2  train (23600/35796)  loss = 0.48837  avg_loss = 0.47612
epoch no.2  train (23800/35796)  loss = 0.56169  avg_loss = 0.46652
epoch no.2  train (24000/35796)  loss = 0.42509  avg_loss = 0.48254
epoch no.2  train (24200/35796)  loss = 0.69038  avg_loss = 0.49606
epoch no.2  train (24400/35796)  loss = 0.56582  avg_loss = 0.49571
epoch no.2  train (24600/35796)  loss = 0.57038  avg_loss = 0.48296
epoch no.2  train (24800/35796)  loss = 0.47713  avg_loss = 0.47838
epoch no.2  train (25000/35796)  loss = 0.23247  avg_loss = 0.47636
epoch no.2  train (25200/35796)  loss = 0.35261  avg_loss = 0.48434
epoch no.2  train (25400/35796)  loss = 0.46529  avg_loss = 0.47432
epoch no.2  train (25600/35796)  loss = 0.48457  avg_loss = 0.47950
epoch no.2  train (25800/35796)  loss = 0.41975  avg_loss = 0.46707
epoch no.2  train (26000/35796)  loss = 0.40990  avg_loss = 0.46810
epoch no.2  train (26200/35796)  loss = 0.40120  avg_loss = 0.47352
epoch no.2  train (26400/35796)  loss = 0.47461  avg_loss = 0.48251
epoch no.2  train (26600/35796)  loss = 0.37997  avg_loss = 0.46109
epoch no.2  train (26800/35796)  loss = 0.34172  avg_loss = 0.46500
epoch no.2  train (27000/35796)  loss = 0.45438  avg_loss = 0.47390
epoch no.2  train (27200/35796)  loss = 0.61299  avg_loss = 0.47882
epoch no.2  train (27400/35796)  loss = 0.33782  avg_loss = 0.47342
epoch no.2  train (27600/35796)  loss = 0.42954  avg_loss = 0.47534
epoch no.2  train (27800/35796)  loss = 0.27133  avg_loss = 0.46713
epoch no.2  train (28000/35796)  loss = 0.34137  avg_loss = 0.46857
epoch no.2  train (28200/35796)  loss = 0.51988  avg_loss = 0.47344
epoch no.2  train (28400/35796)  loss = 0.33247  avg_loss = 0.47987
epoch no.2  train (28600/35796)  loss = 0.59674  avg_loss = 0.47756
epoch no.2  train (28800/35796)  loss = 0.32035  avg_loss = 0.47749
epoch no.2  train (29000/35796)  loss = 0.37525  avg_loss = 0.47744
epoch no.2  train (29200/35796)  loss = 0.48009  avg_loss = 0.47656
epoch no.2  train (29400/35796)  loss = 0.61519  avg_loss = 0.46917
epoch no.2  train (29600/35796)  loss = 0.58908  avg_loss = 0.47997
epoch no.2  train (29800/35796)  loss = 0.47085  avg_loss = 0.47774
epoch no.2  train (30000/35796)  loss = 0.55266  avg_loss = 0.47895
epoch no.2  train (30200/35796)  loss = 0.53214  avg_loss = 0.47397
epoch no.2  train (30400/35796)  loss = 0.50208  avg_loss = 0.47497
epoch no.2  train (30600/35796)  loss = 0.48143  avg_loss = 0.47662
epoch no.2  train (30800/35796)  loss = 0.48725  avg_loss = 0.46410
epoch no.2  train (31000/35796)  loss = 0.61978  avg_loss = 0.47363
epoch no.2  train (31200/35796)  loss = 0.54853  avg_loss = 0.48426
epoch no.2  train (31400/35796)  loss = 0.35351  avg_loss = 0.48698
epoch no.2  train (31600/35796)  loss = 0.31147  avg_loss = 0.47017
epoch no.2  train (31800/35796)  loss = 0.57675  avg_loss = 0.46979
epoch no.2  train (32000/35796)  loss = 0.63652  avg_loss = 0.49273
epoch no.2  train (32200/35796)  loss = 0.36312  avg_loss = 0.46711
epoch no.2  train (32400/35796)  loss = 0.54391  avg_loss = 0.46946
epoch no.2  train (32600/35796)  loss = 0.36949  avg_loss = 0.47686
epoch no.2  train (32800/35796)  loss = 0.44461  avg_loss = 0.48256
epoch no.2  train (33000/35796)  loss = 0.44860  avg_loss = 0.48261
epoch no.2  train (33200/35796)  loss = 0.42495  avg_loss = 0.47460
epoch no.2  train (33400/35796)  loss = 0.35433  avg_loss = 0.49022
epoch no.2  train (33600/35796)  loss = 0.36621  avg_loss = 0.47503
epoch no.2  train (33800/35796)  loss = 0.43449  avg_loss = 0.47666
epoch no.2  train (34000/35796)  loss = 0.36076  avg_loss = 0.48264
epoch no.2  train (34200/35796)  loss = 0.62679  avg_loss = 0.48698
epoch no.2  train (34400/35796)  loss = 0.50700  avg_loss = 0.49133
epoch no.2  train (34600/35796)  loss = 0.42971  avg_loss = 0.47481
epoch no.2  train (34800/35796)  loss = 0.49705  avg_loss = 0.47666
epoch no.2  train (35000/35796)  loss = 0.52042  avg_loss = 0.47248
epoch no.2  train (35200/35796)  loss = 0.50210  avg_loss = 0.47879
epoch no.2  train (35400/35796)  loss = 0.33456  avg_loss = 0.47644
epoch no.2  train (35600/35796)  loss = 0.43625  avg_loss = 0.46964
epoch no.3  train (0/35796)  loss = 0.55965  avg_loss = 0.47767
epoch no.3  train (200/35796)  loss = 0.54610  avg_loss = 0.42186
epoch no.3  train (400/35796)  loss = 0.31331  avg_loss = 0.39927
epoch no.3  train (600/35796)  loss = 0.48638  avg_loss = 0.39409
epoch no.3  train (800/35796)  loss = 0.40282  avg_loss = 0.40487
epoch no.3  train (1000/35796)  loss = 0.38874  avg_loss = 0.40790
epoch no.3  train (1200/35796)  loss = 0.34491  avg_loss = 0.40080
epoch no.3  train (1400/35796)  loss = 0.36478  avg_loss = 0.40992
epoch no.3  train (1600/35796)  loss = 0.39434  avg_loss = 0.40999
epoch no.3  train (1800/35796)  loss = 0.39072  avg_loss = 0.40476
epoch no.3  train (2000/35796)  loss = 0.46237  avg_loss = 0.40961
epoch no.3  train (2200/35796)  loss = 0.40767  avg_loss = 0.40480
epoch no.3  train (2400/35796)  loss = 0.29323  avg_loss = 0.39373
epoch no.3  train (2600/35796)  loss = 0.29553  avg_loss = 0.40133
epoch no.3  train (2800/35796)  loss = 0.46006  avg_loss = 0.40921
epoch no.3  train (3000/35796)  loss = 0.24675  avg_loss = 0.40186
epoch no.3  train (3200/35796)  loss = 0.33772  avg_loss = 0.40191
epoch no.3  train (3400/35796)  loss = 0.33227  avg_loss = 0.40723
epoch no.3  train (3600/35796)  loss = 0.34436  avg_loss = 0.40114
epoch no.3  train (3800/35796)  loss = 0.50303  avg_loss = 0.41121
epoch no.3  train (4000/35796)  loss = 0.27931  avg_loss = 0.40476
epoch no.3  train (4200/35796)  loss = 0.52300  avg_loss = 0.41323
epoch no.3  train (4400/35796)  loss = 0.46852  avg_loss = 0.40204
epoch no.3  train (4600/35796)  loss = 0.45237  avg_loss = 0.40630
epoch no.3  train (4800/35796)  loss = 0.42909  avg_loss = 0.41286
epoch no.3  train (5000/35796)  loss = 0.35948  avg_loss = 0.41721
epoch no.3  train (5200/35796)  loss = 0.49767  avg_loss = 0.40324
epoch no.3  train (5400/35796)  loss = 0.40386  avg_loss = 0.40294
epoch no.3  train (5600/35796)  loss = 0.48471  avg_loss = 0.40659
epoch no.3  train (5800/35796)  loss = 0.40621  avg_loss = 0.39960
epoch no.3  train (6000/35796)  loss = 0.36396  avg_loss = 0.40879
epoch no.3  train (6200/35796)  loss = 0.30959  avg_loss = 0.41075
epoch no.3  train (6400/35796)  loss = 0.33716  avg_loss = 0.40710
epoch no.3  train (6600/35796)  loss = 0.63726  avg_loss = 0.42080
epoch no.3  train (6800/35796)  loss = 0.40199  avg_loss = 0.42160
epoch no.3  train (7000/35796)  loss = 0.17222  avg_loss = 0.41414
epoch no.3  train (7200/35796)  loss = 0.39951  avg_loss = 0.40290
epoch no.3  train (7400/35796)  loss = 0.46371  avg_loss = 0.41466
epoch no.3  train (7600/35796)  loss = 0.42489  avg_loss = 0.41308
epoch no.3  train (7800/35796)  loss = 0.33763  avg_loss = 0.40912
epoch no.3  train (8000/35796)  loss = 0.31084  avg_loss = 0.41047
epoch no.3  train (8200/35796)  loss = 0.41421  avg_loss = 0.40757
epoch no.3  train (8400/35796)  loss = 0.30834  avg_loss = 0.40952
epoch no.3  train (8600/35796)  loss = 0.37614  avg_loss = 0.40074
epoch no.3  train (8800/35796)  loss = 0.29245  avg_loss = 0.41030
epoch no.3  train (9000/35796)  loss = 0.45723  avg_loss = 0.40542
epoch no.3  train (9200/35796)  loss = 0.35421  avg_loss = 0.41432
epoch no.3  train (9400/35796)  loss = 0.33895  avg_loss = 0.40759
epoch no.3  train (9600/35796)  loss = 0.43561  avg_loss = 0.41342
epoch no.3  train (9800/35796)  loss = 0.43827  avg_loss = 0.41115
epoch no.3  train (10000/35796)  loss = 0.40862  avg_loss = 0.41958
epoch no.3  train (10200/35796)  loss = 0.54212  avg_loss = 0.42071
epoch no.3  train (10400/35796)  loss = 0.29741  avg_loss = 0.40574
epoch no.3  train (10600/35796)  loss = 0.57405  avg_loss = 0.41317
epoch no.3  train (10800/35796)  loss = 0.39499  avg_loss = 0.41191
epoch no.3  train (11000/35796)  loss = 0.30066  avg_loss = 0.42294
epoch no.3  train (11200/35796)  loss = 0.41071  avg_loss = 0.41074
epoch no.3  train (11400/35796)  loss = 0.46982  avg_loss = 0.41244
epoch no.3  train (11600/35796)  loss = 0.35130  avg_loss = 0.41182
epoch no.3  train (11800/35796)  loss = 0.44333  avg_loss = 0.41004
epoch no.3  train (12000/35796)  loss = 0.30765  avg_loss = 0.41326
epoch no.3  train (12200/35796)  loss = 0.19477  avg_loss = 0.40659
epoch no.3  train (12400/35796)  loss = 0.39933  avg_loss = 0.41828
epoch no.3  train (12600/35796)  loss = 0.41712  avg_loss = 0.41986
epoch no.3  train (12800/35796)  loss = 0.30158  avg_loss = 0.41229
epoch no.3  train (13000/35796)  loss = 0.49266  avg_loss = 0.40799
epoch no.3  train (13200/35796)  loss = 0.44819  avg_loss = 0.41893
epoch no.3  train (13400/35796)  loss = 0.51509  avg_loss = 0.40844
epoch no.3  train (13600/35796)  loss = 0.41183  avg_loss = 0.40951
epoch no.3  train (13800/35796)  loss = 0.22064  avg_loss = 0.42432
epoch no.3  train (14000/35796)  loss = 0.41148  avg_loss = 0.41904
epoch no.3  train (14200/35796)  loss = 0.38206  avg_loss = 0.42734/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(

epoch no.3  train (14400/35796)  loss = 0.63205  avg_loss = 0.42195
epoch no.3  train (14600/35796)  loss = 0.43084  avg_loss = 0.42153
epoch no.3  train (14800/35796)  loss = 0.44724  avg_loss = 0.42275
epoch no.3  train (15000/35796)  loss = 0.54277  avg_loss = 0.41352
epoch no.3  train (15200/35796)  loss = 0.35394  avg_loss = 0.41439
epoch no.3  train (15400/35796)  loss = 0.46567  avg_loss = 0.41690
epoch no.3  train (15600/35796)  loss = 0.35090  avg_loss = 0.41919
epoch no.3  train (15800/35796)  loss = 0.37886  avg_loss = 0.41881
epoch no.3  train (16000/35796)  loss = 0.31404  avg_loss = 0.42500
epoch no.3  train (16200/35796)  loss = 0.37465  avg_loss = 0.40913
epoch no.3  train (16400/35796)  loss = 0.57357  avg_loss = 0.40924
epoch no.3  train (16600/35796)  loss = 0.45627  avg_loss = 0.41690
epoch no.3  train (16800/35796)  loss = 0.43990  avg_loss = 0.42011
epoch no.3  train (17000/35796)  loss = 0.40604  avg_loss = 0.41367
epoch no.3  train (17200/35796)  loss = 0.27968  avg_loss = 0.41540
epoch no.3  train (17400/35796)  loss = 0.66036  avg_loss = 0.41343
epoch no.3  train (17600/35796)  loss = 0.53204  avg_loss = 0.42629
epoch no.3  train (17800/35796)  loss = 0.35308  avg_loss = 0.41797
epoch no.3  train (18000/35796)  loss = 0.39883  avg_loss = 0.41622
epoch no.3  train (18200/35796)  loss = 0.31297  avg_loss = 0.40821
epoch no.3  train (18400/35796)  loss = 0.44867  avg_loss = 0.41434
epoch no.3  train (18600/35796)  loss = 0.38393  avg_loss = 0.42078
epoch no.3  train (18800/35796)  loss = 0.43367  avg_loss = 0.41968
epoch no.3  train (19000/35796)  loss = 0.36475  avg_loss = 0.41383
epoch no.3  train (19200/35796)  loss = 0.42220  avg_loss = 0.40663
epoch no.3  train (19400/35796)  loss = 0.67167  avg_loss = 0.42162
epoch no.3  train (19600/35796)  loss = 0.38842  avg_loss = 0.41308
epoch no.3  train (19800/35796)  loss = 0.38999  avg_loss = 0.41811
epoch no.3  train (20000/35796)  loss = 0.34042  avg_loss = 0.41497
epoch no.3  train (20200/35796)  loss = 0.42966  avg_loss = 0.41887
epoch no.3  train (20400/35796)  loss = 0.34326  avg_loss = 0.41517
epoch no.3  train (20600/35796)  loss = 0.27664  avg_loss = 0.41799
epoch no.3  train (20800/35796)  loss = 0.28118  avg_loss = 0.40410
epoch no.3  train (21000/35796)  loss = 0.46561  avg_loss = 0.42007
epoch no.3  train (21200/35796)  loss = 0.41511  avg_loss = 0.41668
epoch no.3  train (21400/35796)  loss = 0.39039  avg_loss = 0.41397
epoch no.3  train (21600/35796)  loss = 0.38948  avg_loss = 0.41256
epoch no.3  train (21800/35796)  loss = 0.23866  avg_loss = 0.42558
epoch no.3  train (22000/35796)  loss = 0.41825  avg_loss = 0.41954
epoch no.3  train (22200/35796)  loss = 0.45080  avg_loss = 0.42014
epoch no.3  train (22400/35796)  loss = 0.43790  avg_loss = 0.41681
epoch no.3  train (22600/35796)  loss = 0.51578  avg_loss = 0.41516
epoch no.3  train (22800/35796)  loss = 0.62933  avg_loss = 0.42713
epoch no.3  train (23000/35796)  loss = 0.41407  avg_loss = 0.42436
epoch no.3  train (23200/35796)  loss = 0.33693  avg_loss = 0.42406
epoch no.3  train (23400/35796)  loss = 0.37523  avg_loss = 0.41626
epoch no.3  train (23600/35796)  loss = 0.36318  avg_loss = 0.41584
epoch no.3  train (23800/35796)  loss = 0.54479  avg_loss = 0.42430
epoch no.3  train (24000/35796)  loss = 0.41665  avg_loss = 0.43240
epoch no.3  train (24200/35796)  loss = 0.34018  avg_loss = 0.42368
epoch no.3  train (24400/35796)  loss = 0.33160  avg_loss = 0.41907
epoch no.3  train (24600/35796)  loss = 0.56527  avg_loss = 0.43708
epoch no.3  train (24800/35796)  loss = 0.49108  avg_loss = 0.42188
epoch no.3  train (25000/35796)  loss = 0.52592  avg_loss = 0.41678
epoch no.3  train (25200/35796)  loss = 0.42621  avg_loss = 0.42065
epoch no.3  train (25400/35796)  loss = 0.51610  avg_loss = 0.42380
epoch no.3  train (25600/35796)  loss = 0.50114  avg_loss = 0.42881
epoch no.3  train (25800/35796)  loss = 0.48616  avg_loss = 0.41177
epoch no.3  train (26000/35796)  loss = 0.32079  avg_loss = 0.41488
epoch no.3  train (26200/35796)  loss = 0.54024  avg_loss = 0.41474
epoch no.3  train (26400/35796)  loss = 0.49280  avg_loss = 0.42173
epoch no.3  train (26600/35796)  loss = 0.42639  avg_loss = 0.42695
epoch no.3  train (26800/35796)  loss = 0.20617  avg_loss = 0.41608
epoch no.3  train (27000/35796)  loss = 0.44923  avg_loss = 0.42270
epoch no.3  train (27200/35796)  loss = 0.44118  avg_loss = 0.42698
epoch no.3  train (27400/35796)  loss = 0.34711  avg_loss = 0.41488
epoch no.3  train (27600/35796)  loss = 0.54319  avg_loss = 0.40481
epoch no.3  train (27800/35796)  loss = 0.46629  avg_loss = 0.43168
epoch no.3  train (28000/35796)  loss = 0.47742  avg_loss = 0.41040
epoch no.3  train (28200/35796)  loss = 0.49021  avg_loss = 0.41965
epoch no.3  train (28400/35796)  loss = 0.75655  avg_loss = 0.43024
epoch no.3  train (28600/35796)  loss = 0.41694  avg_loss = 0.42027
epoch no.3  train (28800/35796)  loss = 0.58274  avg_loss = 0.42567
epoch no.3  train (29000/35796)  loss = 0.55741  avg_loss = 0.42445
epoch no.3  train (29200/35796)  loss = 0.37899  avg_loss = 0.42114
epoch no.3  train (29400/35796)  loss = 0.38945  avg_loss = 0.42716
epoch no.3  train (29600/35796)  loss = 0.46705  avg_loss = 0.42596
epoch no.3  train (29800/35796)  loss = 0.42414  avg_loss = 0.41715
epoch no.3  train (30000/35796)  loss = 0.38012  avg_loss = 0.42766
epoch no.3  train (30200/35796)  loss = 0.46430  avg_loss = 0.40908
epoch no.3  train (30400/35796)  loss = 0.42585  avg_loss = 0.41766
epoch no.3  train (30600/35796)  loss = 0.36991  avg_loss = 0.41652
epoch no.3  train (30800/35796)  loss = 0.40486  avg_loss = 0.41906
epoch no.3  train (31000/35796)  loss = 0.20459  avg_loss = 0.41677
epoch no.3  train (31200/35796)  loss = 0.56159  avg_loss = 0.42222
epoch no.3  train (31400/35796)  loss = 0.55825  avg_loss = 0.42885
epoch no.3  train (31600/35796)  loss = 0.48993  avg_loss = 0.42261
epoch no.3  train (31800/35796)  loss = 0.30560  avg_loss = 0.42262
epoch no.3  train (32000/35796)  loss = 0.43489  avg_loss = 0.41764
epoch no.3  train (32200/35796)  loss = 0.29649  avg_loss = 0.42830
epoch no.3  train (32400/35796)  loss = 0.31316  avg_loss = 0.41987
epoch no.3  train (32600/35796)  loss = 0.40369  avg_loss = 0.43003
epoch no.3  train (32800/35796)  loss = 0.62870  avg_loss = 0.44346
epoch no.3  train (33000/35796)  loss = 0.63038  avg_loss = 0.43223
epoch no.3  train (33200/35796)  loss = 0.39236  avg_loss = 0.42791
epoch no.3  train (33400/35796)  loss = 0.55050  avg_loss = 0.42682
epoch no.3  train (33600/35796)  loss = 0.49622  avg_loss = 0.43359
epoch no.3  train (33800/35796)  loss = 0.30818  avg_loss = 0.42792
epoch no.3  train (34000/35796)  loss = 0.41209  avg_loss = 0.42156
epoch no.3  train (34200/35796)  loss = 0.45044  avg_loss = 0.42499
epoch no.3  train (34400/35796)  loss = 0.51830  avg_loss = 0.41581
epoch no.3  train (34600/35796)  loss = 0.34016  avg_loss = 0.42046
epoch no.3  train (34800/35796)  loss = 0.41859  avg_loss = 0.42227
epoch no.3  train (35000/35796)  loss = 0.48652  avg_loss = 0.42460
epoch no.3  train (35200/35796)  loss = 0.48024  avg_loss = 0.42564
epoch no.3  train (35400/35796)  loss = 0.43597  avg_loss = 0.43936
epoch no.3  train (35600/35796)  loss = 0.70649  avg_loss = 0.44546
